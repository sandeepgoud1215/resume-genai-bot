import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import pipeline

# âœ… Page settings
st.set_page_config(page_title="Smart Resume Q&A Bot", layout="wide")
st.title("ğŸ§  Resume Q&A Bot â€“ GenAI Powered")

# âœ… Load models (only once)
@st.cache_resource
def load_models():
    embed_model = SentenceTransformer("all-MiniLM-L6-v2")
    qa_model = pipeline("text2text-generation", model="google/flan-t5-base")
    return embed_model, qa_model

embed_model, qa_model = load_models()

# âœ… File upload
uploaded_file = st.file_uploader("ğŸ“„ Upload your Resume (PDF)", type="pdf")

if uploaded_file:
    with open("resume.pdf", "wb") as f:
        f.write(uploaded_file.read())
    st.success("âœ… Resume uploaded!")

    # âœ… Extract text from resume
    with fitz.open("resume.pdf") as doc:
        resume_text = " ".join([page.get_text() for page in doc])

    # âœ… Chunk text
    CHUNK_SIZE = 300
    chunks = [resume_text[i:i+CHUNK_SIZE] for i in range(0, len(resume_text), CHUNK_SIZE)]

    # âœ… Generate embeddings and store in FAISS index
    embeddings = embed_model.encode(chunks)
    index = faiss.IndexFlatL2(embeddings[0].shape[0])
    index.add(np.array(embeddings))

    # âœ… Ask a question
    question = st.text_input("Ask your question about the resume ğŸ‘‡")

    # âœ… Show example questions
    st.markdown("#### ğŸ’¬ Example Questions:")
    st.markdown("""
    - âœ… Is this resume suitable for a Data Analyst role?
    - ğŸ’¼ What roles is this resume best suited for?
    - ğŸ§  What are the candidateâ€™s top skills?
    - ğŸ§ª Does the candidate know Python or SQL?
    - ğŸ“ What is the educational background?
    - ğŸ“Š Are any data analysis projects mentioned?
    - ğŸ”§ What tools or technologies are listed?
    - ğŸ” Does this resume include internship or job experience?
    - ğŸ“ How can this resume be improved?
    """)

    if question:
        # âœ… Retrieve top 3 most relevant chunks
        q_embedding = embed_model.encode([question])
        D, I = index.search(np.array(q_embedding), k=3)
        best_chunk = " ".join([chunks[i] for i in I[0]])

        # âœ… Build smart GenAI prompt
        prompt = (
            "You are an intelligent assistant helping review resumes.\n"
            f"Here is a section of the resume:\n{best_chunk}\n\n"
            f"Question: {question}\n\n"
            "Answer:"
        )

        # âœ… Generate answer with flan-t5
        with st.spinner("ğŸ¤– Thinking..."):
            answer = qa_model(prompt, max_new_tokens=100)[0]['generated_text']

        # âœ… Show answer
        st.markdown("### ğŸ§  Answer:")
        st.success(answer)
